{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusterless Decoding\n",
    "\n",
    "## Overview\n",
    "\n",
    "_Developer Note:_ if you may make a PR in the future, be sure to copy this\n",
    "notebook, and use the `gitignore` prefix `temp` to avoid future conflicts.\n",
    "\n",
    "This is one notebook in a multi-part series on Spyglass.\n",
    "\n",
    "- To set up your Spyglass environment and database, see\n",
    "  [the Setup notebook](./00_Setup.ipynb)\n",
    "- This tutorial assumes you've already \n",
    "  [extracted waveforms](./41_Extracting_Clusterless_Waveform_Features.ipynb), as well as loaded \n",
    "  [position data](./20_Position_Trodes.ipynb). If 1D decoding, this data should also be\n",
    "  [linearized](./24_Linearization.ipynb).\n",
    "\n",
    "Clusterless decoding can be performed on either 1D or 2D data. We will start with 2D data.\n",
    "\n",
    "## Elements of Clusterless Decoding\n",
    "- **Position Data**: This is the data that we want to decode. It can be 1D or 2D.\n",
    "- **Spike Waveform Features**: These are the features that we will use to decode the position data.\n",
    "- **Decoding Model Parameters**: This is how we define the model that we will use to decode the position data.\n",
    "\n",
    "## Grouping Data\n",
    "An important concept will be groups. Groups are tables that allow use to specify collections of data. We will use groups in two situations here:\n",
    "1. Because we want to decode from more than one tetrode (or probe), so we will create a group that contains all of the tetrodes that we want to decode from. \n",
    "2. Similarly, we will create a group for the position data that we want to decode, so that we can decode from position data from multiple sessions.\n",
    "\n",
    "### Grouping Waveform Features\n",
    "Let's start with grouping the Waveform Features. We will first inspect the waveform features that we have extracted to figure out the primary keys of the data that we want to decode from. We need to use the tables `SpikeSortingSelection` and `SpikeSortingOutput` to figure out the `merge_id` associated with `nwb_file_name` to get the waveform features associated with the NWB file of interest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import datajoint as dj\n",
    "\n",
    "dj.config.load(\n",
    "    Path(\"../dj_local_conf.json\").absolute()\n",
    ")  # load config for database connection info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spyglass.spikesorting.merge import SpikeSortingOutput\n",
    "import spyglass.spikesorting.v1 as sgs\n",
    "from spyglass.decoding.v1.waveform_features import UnitWaveformFeaturesSelection\n",
    "\n",
    "\n",
    "nwb_copy_file_name = \"mediumnwb20230802_.nwb\"\n",
    "\n",
    "sorter_keys = {\n",
    "    \"nwb_file_name\": nwb_copy_file_name,\n",
    "    \"sorter\": \"clusterless_thresholder\",\n",
    "    \"sorter_param_name\": \"default_clusterless\",\n",
    "}\n",
    "\n",
    "feature_key = {\"features_param_name\": \"amplitude\"}\n",
    "\n",
    "(sgs.SpikeSortingSelection & sorter_keys) * SpikeSortingOutput.CurationV1 * (\n",
    "    UnitWaveformFeaturesSelection.proj(merge_id=\"spikesorting_merge_id\")\n",
    "    & feature_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spyglass.decoding.v1.waveform_features import UnitWaveformFeaturesSelection\n",
    "\n",
    "spikesorting_merge_id = (\n",
    "    (sgs.SpikeSortingSelection & sorter_keys)\n",
    "    * SpikeSortingOutput.CurationV1\n",
    "    * (\n",
    "        UnitWaveformFeaturesSelection.proj(merge_id=\"spikesorting_merge_id\")\n",
    "        & feature_key\n",
    "    )\n",
    ").fetch(\"merge_id\")\n",
    "\n",
    "waveform_selection_keys = [\n",
    "    {\"spikesorting_merge_id\": merge_id, \"features_param_name\": \"amplitude\"}\n",
    "    for merge_id in spikesorting_merge_id\n",
    "]\n",
    "\n",
    "UnitWaveformFeaturesSelection & waveform_selection_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a group called `test_group` that contains all of the tetrodes that we want to decode from. We will use the `create_group` function to create this group. This function takes two arguments: the name of the group, and the keys of the tables that we want to include in the group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spyglass.decoding.v1.clusterless import UnitWaveformFeaturesGroup\n",
    "\n",
    "UnitWaveformFeaturesGroup().create_group(\"test_group\", waveform_selection_keys)\n",
    "UnitWaveformFeaturesGroup & {\"waveform_features_group_name\": \"test_group\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we successfully associated \"test_group\" with the tetrodes that we want to decode from by using the `get_group` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UnitWaveformFeaturesGroup.UnitFeatures & {\n",
    "    \"waveform_features_group_name\": \"test_group\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping Position Data\n",
    "\n",
    "We will now create a group called `02_r1` that contains all of the position data that we want to decode from. As before, we will use the `create_group` function to create this group. This function takes two arguments: the name of the group, and the keys of the tables that we want to include in the group.\n",
    "\n",
    "We use the the `PositionOutput` table to figure out the `merge_id` associated with `nwb_file_name` to get the position data associated with the NWB file of interest. In this case, we only have one position to insert, but we could insert multiple positions if we wanted to decode from multiple sessions.\n",
    "\n",
    "Note that the position data sampling frequency is what determines the time step of the decoding. In this case, the position data sampling frequency is 30 Hz, so the time step of the decoding will be 1/30 seconds. In practice, you will want to use a smaller time step such as 500 Hz. This will allow you to decode at a finer time scale. To do this, you will want to interpolate the position data to a higher sampling frequency as shown in the [position trodes notebook](./20_Position_Trodes.ipynb).\n",
    "\n",
    "You will also want to specify the name of the position variables if they are different from the default names. The default names are `position_x` and `position_y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spyglass.position import PositionOutput\n",
    "\n",
    "PositionOutput.TrodesPosV1 & {\"nwb_file_name\": nwb_copy_file_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spyglass.decoding.v1.clusterless import PositionGroup\n",
    "\n",
    "position_merge_ids = (\n",
    "    PositionOutput.TrodesPosV1\n",
    "    & {\n",
    "        \"nwb_file_name\": nwb_copy_file_name,\n",
    "        \"interval_list_name\": \"pos 0 valid times\",\n",
    "        \"trodes_pos_params_name\": \"default\",\n",
    "    }\n",
    ").fetch(\"merge_id\")\n",
    "\n",
    "PositionGroup().create_group(\n",
    "    \"test_group\",\n",
    "    [{\"pos_merge_id\": merge_id} for merge_id in position_merge_ids],\n",
    ")\n",
    "\n",
    "PositionGroup & {\"position_group_name\": \"test_group\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(PositionGroup & {\"position_group_name\": \"test_group\"}).fetch1(\n",
    "    \"position_variables\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PositionGroup.Position & {\"position_group_name\": \"test_group\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding Model Parameters\n",
    "\n",
    "We will use the `non_local_detector` package to decode the data. This package is highly flexible and allows several different types of models to be used. In this case, we will use the `ContFragClusterlessClassifier` to decode the data. This has two discrete states: Continuous and Fragmented, which correspond to different types of movement models. To read more about this model, see:\n",
    "> Denovellis, E.L., Gillespie, A.K., Coulter, M.E., Sosa, M., Chung, J.E., Eden, U.T., and Frank, L.M. (2021). Hippocampal replay of experience at real-world speeds. eLife 10, e64505. [10.7554/eLife.64505](https://doi.org/10.7554/eLife.64505).\n",
    "\n",
    "Let's first look at the model and the default parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from non_local_detector.models import ContFragClusterlessClassifier\n",
    "\n",
    "ContFragClusterlessClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change these parameters like so: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from non_local_detector.models import ContFragClusterlessClassifier\n",
    "\n",
    "ContFragClusterlessClassifier(\n",
    "    clusterless_algorithm_params={\n",
    "        \"block_size\": 10000,\n",
    "        \"position_std\": 12.0,\n",
    "        \"waveform_std\": 24.0,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To insert these parameters into the database, we need to use the following syntax, we need to convert the initialized model into a dictionary, and then insert the dictionary into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(ContFragClusterlessClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spyglass.decoding.v1.core import DecodingParameters\n",
    "\n",
    "\n",
    "DecodingParameters.insert1(\n",
    "    {\n",
    "        \"decoding_param_name\": \"contfrag_clusterless\",\n",
    "        \"decoding_params\": vars(ContFragClusterlessClassifier()),\n",
    "        \"decoding_kwargs\": dict(),\n",
    "    },\n",
    "    skip_duplicates=True,\n",
    ")\n",
    "\n",
    "DecodingParameters & {\"decoding_param_name\": \"contfrag_clusterless\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve these parameters and rebuild the model like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = (\n",
    "    DecodingParameters & {\"decoding_param_name\": \"contfrag_clusterless\"}\n",
    ").fetch1()\n",
    "\n",
    "ContFragClusterlessClassifier(**model_params[\"decoding_params\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D Decoding\n",
    "\n",
    "If you want to do 1D decoding, you will need to specify the `track_graph`, `edge_order`, and `edge_spacing` in the `environments` parameter. You can read more about these parameters in the [linearization notebook](./24_Linearization.ipynb). You can retrieve these parameters from the `TrackGraph` table if you have stored them there. These will then go into the `environments` parameter of the `ContFragClusterlessClassifier` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from non_local_detector.environment import Environment\n",
    "\n",
    "Environment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding\n",
    "\n",
    "Now that we have grouped the data and defined the model parameters, we have finally set up the elements in tables that we need to decode the data. We now need to use the `ClusterlessDecodingSelection` to fully specify all the parameters and data that we want.\n",
    "\n",
    "This has:\n",
    "- `waveform_features_group_name`: the name of the group that contains the waveform features that we want to decode from\n",
    "- `position_group_name`: the name of the group that contains the position data that we want to decode from\n",
    "- `decoding_param_name`: the name of the decoding parameters that we want to use\n",
    "- `nwb_file_name`: the name of the NWB file that we want to decode from\n",
    "- `encoding_interval`: the interval of time that we want to train the initial model on\n",
    "- `decoding_interval`: the interval of time that we want to decode from\n",
    "- `estimate_decoding_params`: whether or not we want to estimate the decoding parameters\n",
    "\n",
    "\n",
    "The first three parameters should be familiar to you. \n",
    "\n",
    "\n",
    "### Decoding and Encoding Intervals\n",
    "The `encoding_interval` is the interval of time that we want to train the initial model on. The `decoding_interval` is the interval of time that we want to decode from. These two intervals can be the same, but they do not have to be. For example, we may want to train the model on a long interval of time, but only decode from a short interval of time. This is useful if we want to decode from a short interval of time that is not representative of the entire session. In this case, we will train the model on a longer interval of time that is representative of the entire session.\n",
    "\n",
    "These keys come from the `IntervalList` table. We can see that the `IntervalList` table contains the `nwb_file_name` and `interval_name` that we need to specify the `encoding_interval` and `decoding_interval`. We will specify a short decoding interval called `test decoding interval` and use that to decode from.\n",
    "\n",
    "\n",
    "### Estimating Decoding Parameters\n",
    "The last parameter is `estimate_decoding_params`. This is a boolean that specifies whether or not we want to estimate the decoding parameters. If this is `True`, then we will estimate the initial conditions and discrete transition matrix from the data.\n",
    "\n",
    "NOTE: If estimating parameters, then we need to treat times outside decoding interval as missing. this means that times outside the decoding interval will not use the spiking data and only the state transition matrix and previous time step will be used. This may or may not be desired depending on the length of this missing interval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spyglass.decoding.v1.clusterless import ClusterlessDecodingSelection\n",
    "\n",
    "ClusterlessDecodingSelection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spyglass.common import IntervalList\n",
    "\n",
    "IntervalList & {\"nwb_file_name\": nwb_copy_file_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding_interval_valid_times = [\n",
    "    [1625935714.6359036, 1625935714.6359036 + 15.0]\n",
    "]\n",
    "\n",
    "IntervalList.insert1(\n",
    "    {\n",
    "        \"nwb_file_name\": \"mediumnwb20230802_.nwb\",\n",
    "        \"interval_list_name\": \"test decoding interval\",\n",
    "        \"valid_times\": decoding_interval_valid_times,\n",
    "    },\n",
    "    skip_duplicates=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have figured out the keys that we need, we can insert the `ClusterlessDecodingSelection` into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_key = {\n",
    "    \"waveform_features_group_name\": \"test_group\",\n",
    "    \"position_group_name\": \"02_r1\",\n",
    "    \"decoding_param_name\": \"contfrag_clusterless\",\n",
    "    \"nwb_file_name\": \"mediumnwb20230802_.nwb\",\n",
    "    \"encoding_interval\": \"pos 0 valid times\",\n",
    "    \"decoding_interval\": \"test decoding interval\",\n",
    "    \"estimate_decoding_params\": False,\n",
    "}\n",
    "\n",
    "ClusterlessDecodingSelection.insert1(\n",
    "    selection_key,\n",
    "    skip_duplicates=True,\n",
    ")\n",
    "\n",
    "ClusterlessDecodingSelection & selection_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run decoding, we simply populate the `ClusterlessDecodingOutput` table. This will run the decoding and insert the results into the database. We can then retrieve the results from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spyglass.decoding.v1.clusterless import ClusterlessDecodingV1\n",
    "\n",
    "ClusterlessDecodingV1.populate(selection_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see it as an entry in the `DecodingOutput` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spyglass.decoding.decoding_merge import DecodingOutput\n",
    "\n",
    "DecodingOutput.ClusterlessDecodingV1 & selection_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the results of the decoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding_results = (ClusterlessDecodingV1 & selection_key).load_results()\n",
    "decoding_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if we deleted the results, we can use the `cleanup` function to delete the results from the file system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecodingOutput().cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from non_local_detector.visualization import create_interactive_2D_decoding_figurl\n",
    "\n",
    "create_interactive_2D_decoding_figurl(\n",
    "    position_time=,\n",
    "    position=,\n",
    "    results=results,\n",
    "    posterior,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPUs\n",
    "We can use GPUs for decoding which will result in a significant speedup. This is achieved using the [jax](https://jax.readthedocs.io/en/latest/) package.\n",
    "\n",
    "### Ensuring jax can find a GPU\n",
    " Assuming you've set up a GPU, we can use `jax.devices()` to make sure the decoding code can see the GPU. If a GPU is available, it will be listed.\n",
    "\n",
    "In the following instance, we do not have a GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CpuDevice(id=0)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a GPU\n",
    "If you do have multiple GPUs, you can use the `jax` package to set the device (GPU) that you want to use. For example, if you want to use the second GPU, you can use the following code (uncomment first):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device_id = 2\n",
    "# device = jax.devices()[device_id]\n",
    "# jax.config.update(\"jax_default_device\", device)\n",
    "# device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring GPU Usage\n",
    "\n",
    "You can see which GPUs are occupied (if you have multiple GPUs) by running the command `nvidia-smi` in\n",
    "a terminal (or `!nvidia-smi` in a notebook). Pick a GPU with low memory usage. \n",
    "\n",
    "We can monitor GPU use with the terminal command `watch -n 0.1 nvidia-smi`, will\n",
    "update `nvidia-smi` every 100 ms. This won't work in a notebook, as it won't\n",
    "display the updates.\n",
    "\n",
    "Other ways to monitor GPU usage are:\n",
    "\n",
    "- A \n",
    "  [jupyter widget by nvidia](https://github.com/rapidsai/jupyterlab-nvdashboard)\n",
    "  to monitor GPU usage in the notebook\n",
    "- A [terminal program](https://github.com/peci1/nvidia-htop) like nvidia-smi\n",
    "  with more information about  which GPUs are being utilized and by whom.\n",
    "\n",
    "### Parallelizing Decoding\n",
    "\n",
    "You can also use the [dask_cuda](https://docs.rapids.ai/api/dask-cuda/nightly/) to parallelize decoding. You will need to install the `dask_cuda` package (see [here](https://docs.rapids.ai/api/dask-cuda/nightly/install/)). You then can run the following code to parallelize decoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask\n",
    "# from dask.distributed import Client\n",
    "# from dask_cuda import LocalCUDACluster\n",
    "\n",
    "# cluster = LocalCUDACluster()\n",
    "\n",
    "# selection_keys = [] # list of selection keys\n",
    "\n",
    "# with Client(cluster) as client:\n",
    "#     results = [\n",
    "#         dask.delayed(ClusterlessDecodingV1.populate)(\n",
    "#             selection_key, reserve_jobs=True\n",
    "#         )\n",
    "#         for selection_key in selection_keys\n",
    "#     ]\n",
    "#     dask.compute(*results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spyglass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
